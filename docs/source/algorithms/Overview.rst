FinRL provides fine-tuned state-of-the-art DRL algorithms (DQN, DDPG, PPO, SAC, A2C, TD3, etc.), commonly used reward functions and standard evaluation baselines to alleviate the debugging workloads and promote the reproducibility.

1. Different types of action

•	DDPG, TD3, SAC, PPO, PPO (GAE),REDQ for continuous actions
•	DQN, DoubleDQN, D3QN, SAC for discrete actions
•	QMIX, VDN; MADDPG, MAPPO, MATD3 for multi-agent environment

  
2. Single or multiple agent environment

•	DDPG, TD3, SAC, PPO, PPO (GAE),REDQ, DQN, DoubleDQN, D3QN, SAC for single agent environment
•	QMIX, VDN; MADDPG, MAPPO, MATD3 for multi-agent environment

3. Evolution line of DRL algorithms

DRL is always about coming up with state-of-the-art performance algorithms (DQN, DDPG, Policy Gradient, A2C, PPO, TD3, SAC, etc. the advantages and disadvantages for each algorithm, for example, if you think DQN < DDPG < TD3 < SAC)





